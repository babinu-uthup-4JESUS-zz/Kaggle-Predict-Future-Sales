{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "The idea here is to design a gradient boosting prediction algorithm using tensorflow. We will be using estimator API, which allows us seamless transition to other models !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The directory for storing metadata related to the model. This way, we can easily build models step by step, starting\n",
    "# from the previous checkpoint, in cases when we have large amount of data.\n",
    "OUTDIR = 'sample_model_metadata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The pandas input function which is to be fed to the estimator API. NOTE : Since the syntax is very similar, we could \n",
    "# have used one function for training and evaluation as well, but we have decided to keep them separately for the sake\n",
    "# of clarity.\n",
    "def make_train_input_fn(df, num_epochs=1, shuffle=True):\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "        x=df,\n",
    "        y=df['item_cnt_month'],\n",
    "        batch_size=128,\n",
    "        num_epochs=num_epochs,\n",
    "        shuffle=shuffle,\n",
    "        queue_capacity=2000)\n",
    "\n",
    "def make_eval_input_fn(df):\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "        x=df,\n",
    "        y=df['item_cnt_month'],\n",
    "        batch_size=128,\n",
    "        num_epochs=1,\n",
    "        shuffle=False,\n",
    "        queue_capacity=2000)\n",
    "\n",
    "def make_prediction_input_fn(df):\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "        x=df,\n",
    "        batch_size=128,\n",
    "        num_epochs=1,\n",
    "        shuffle=False,\n",
    "        queue_capacity=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "REL_PREDICTOR_COLS = ['date_block_num', 'shop_id', 'item_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This routine makes the input in a tensorflow digestible form.\n",
    "def make_feature_cols():\n",
    "  input_columns = [tf.feature_column.numeric_column(k) for k in REL_PREDICTOR_COLS]\n",
    "  return input_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Get relevant data into pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train = pd.read_csv(\"input/sales_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_sales_data = sales_train[['date_block_num', 'shop_id', 'item_id', 'item_cnt_day' ]].groupby(\n",
    "    ['date_block_num', 'shop_id', 'item_id']).sum()\n",
    "monthly_sales_data.rename(columns={'item_cnt_day':'item_cnt_month'}, inplace=True)\n",
    "monthly_sales_data.reset_index(['date_block_num', 'shop_id', 'item_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date_block_num', 'shop_id', 'item_id', 'item_cnt_month'], dtype='object')"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monthly_sales_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = monthly_sales_data[monthly_sales_data.date_block_num != 32]\n",
    "validation_data = monthly_sales_data[monthly_sales_data.date_block_num == 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Addtitional specific tensorflow functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train any model, for which a tensorflow estimator API is built. This way, we can test out several models with a one\n",
    "# line change.\n",
    "def get_trained_model(model, train_data, outdir=OUTDIR, logging_mode=tf.logging.INFO):\n",
    "    \n",
    "    tf.logging.set_verbosity(logging_mode)\n",
    "    \n",
    "    # Delete the directory corresponding to metadata of model so as to build it from scratch.\n",
    "    shutil.rmtree(outdir, ignore_errors=True)\n",
    "\n",
    "    # Train data for a reasonable number of epochs (100) as default.\n",
    "    model.train(make_train_input_fn(train_data, num_epochs=100, shuffle=False))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_from_model(model, test_data):\n",
    "    predictions = model.predict(input_fn=make_prediction_input_fn(test_data)) \n",
    "    return [x['predictions'][0] for x in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train the model and obtain validation score. Also, make score that the predictions are clipped so that they make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'sample_model_metadata', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x13f5c6f98>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:Issue encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_Resource' object has no attribute 'name'\n",
      "INFO:tensorflow:Saving checkpoints for 0 into sample_model_metadata/model.ckpt.\n",
      "WARNING:tensorflow:Issue encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_Resource' object has no attribute 'name'\n",
      "INFO:tensorflow:loss = 13.0625, step = 0\n",
      "INFO:tensorflow:loss = 2.7483807, step = 99 (1.073 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.2946\n",
      "INFO:tensorflow:loss = 17.070436, step = 199 (0.774 sec)\n",
      "INFO:tensorflow:global_step/sec: 231.837\n",
      "INFO:tensorflow:loss = 18.393755, step = 299 (0.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.869\n",
      "INFO:tensorflow:loss = 2.1596873, step = 399 (0.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.096\n",
      "INFO:tensorflow:loss = 10.851213, step = 499 (0.509 sec)\n",
      "INFO:tensorflow:global_step/sec: 196.412\n",
      "INFO:tensorflow:loss = 55.0658, step = 599 (0.535 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.519\n",
      "INFO:tensorflow:loss = 28.832756, step = 699 (0.562 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.12\n",
      "INFO:tensorflow:loss = 5.6023912, step = 799 (0.583 sec)\n",
      "INFO:tensorflow:global_step/sec: 171.313\n",
      "INFO:tensorflow:loss = 86.23106, step = 899 (0.612 sec)\n",
      "INFO:tensorflow:global_step/sec: 163.206\n",
      "INFO:tensorflow:loss = 6.695802, step = 999 (0.637 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.1\n",
      "INFO:tensorflow:loss = 10.863902, step = 1099 (0.667 sec)\n",
      "INFO:tensorflow:global_step/sec: 149.502\n",
      "INFO:tensorflow:loss = 58.53209, step = 1199 (0.723 sec)\n",
      "INFO:tensorflow:global_step/sec: 138.078\n",
      "INFO:tensorflow:Saving checkpoints for 1200 into sample_model_metadata/model.ckpt.\n",
      "WARNING:tensorflow:Issue encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_Resource' object has no attribute 'name'\n",
      "INFO:tensorflow:Loss for final step: 1.5260549.\n"
     ]
    }
   ],
   "source": [
    "model = get_trained_model(tf.estimator.BoostedTreesRegressor(feature_columns=make_feature_cols(),\n",
    "                                                             n_batches_per_layer=2,\n",
    "                                                             model_dir=OUTDIR),\n",
    "                         train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from sample_model_metadata/model.ckpt-1200\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "model_predictions = get_predictions_from_model(model, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmse(model_predictions, validation_data):\n",
    "    actual_vals = np.clip(validation_data.item_cnt_month.values, 0, 20)\n",
    "    return np.sqrt(mean_squared_error(actual_vals, np.clip(model_predictions, 0, 20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6254074716990776"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_rmse(model_predictions, validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What next ?\n",
    "\n",
    "One option for us would be to add relevant data from other files and see if we can extract something more from this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "shops = pd.read_csv('input/shops.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['shop_name', 'shop_id'], dtype='object')"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shops.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_categories = pd.read_csv('input/item_categories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['item_category_name', 'item_category_id'], dtype='object')"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_categories.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv('input/items.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['item_name', 'item_id', 'item_category_id'], dtype='object')"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use item_category_id as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
